<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>Working memory project</title>
<link rel="stylesheet" type="text/css" href="/css/global.css">
<link rel="stylesheet" type="text/css" href="css/local.css"></head>
<body><div>
    <center>
    	<span style="float: left">
    		<a href="https://rq-chen.github.io/microstate-research/index.html">Before: Microstate research</a>
    	</span>
        <a href="https://rq-chen.github.io/index.html">Homepage</a>
    	<span style="float: right">
    		<a href="https://rq-chen.github.io/traveling-wave/index.html">Next: Traveling wave project</a>
    	</span>
    </center>
</div>
<hr />
<h1 id='undergraduate-research-in-working-memory'>Undergraduate Research in Working Memory</h1>
<p><strong>Update: </strong>I am now working with PhD candidate Ying Fan on an auditory working memory experiment. I simulated a recurrent neural network to explore how frequency and position of stimuli are represented in the network, dependent on the experiment setting. Here are some basic results (details available upon request):</p>
<p><strong>Experiment and Network Design:</strong></p>
<p><img src = "RNN/IO.png" style = "zoom:50%" /></p>
<p>During each trial, participants will hear a sequence of three tones and see a cue, then compare the cued item with a target after a white noise impulse. We manipulated the difficulty of the task and explored the difference in feature representation in the network.</p>
<p>The artificial neural network contains only one LSTM layer and one output layer. The 24-dimensional input is encoded in a (pseudo-) continous attractor for frequency with 20 neurons and 4 extra neurons for visual cue. The input is processed by the LSTM layer and transformed into another 24-dimensional output, then sent to the only one output neuron.</p>
<p><strong>Training: </strong></p>
<p><img src = "RNN/Training.png" style = "zoom:60%"/></p>
<p>Due to RNN&#39;s extremely powerful memory, we have to make sure it &quot;understands&quot; the experiment rather than memorizes each trial. This figure demonstrates that our model successfully utilized cue information and achieved high accuracy.</p>
<p><strong>Encoding:</strong></p>
<p><img src = "RNN/FreqPosCorr1.png" style = "zoom:50%"/></p>
<p>We performed traditional Representational Similarity Analysis (RSA) on the &quot;neural activity&quot; of the network. Here we show the results from 8 neurons, data categorized by (frequency, position) pairs (6 * 3 = 18 groups).</p>
<p><img src = "RNN/TuningCurve1.png" style = "zoom:50%"/></p>
<p>We plotted the tuning curves of each neuron. It&#39;s clear that some neurons only encode frequency (e.g. the bottom-left one) but some encode frequency and position jointly.</p>
<p><img src = "RNN/EncodingTend.png" style = "zoom:50%"/></p>
<p>We explored the proportion of variance explained by frequency or position for each neuron. We can see that most neurons mainly encode frequency, but some encode both.</p>
<p><strong>Stimulation:</strong></p>
<p><img src = "RNN/Cue1.png" style = "zoom:50%"/></p>
<p>We also explored how the network reacted after cue and noise impulse. We see that after the cue, the network still represent frequency and position jointly.</p>
<p><img src = "RNN/Noise1.png" style = "zoom:50%"/></p>
<p>However, after the noise impulse, the information for position is no longer presevered.</p>
<p><strong>Influence of task difficulty:</strong></p>
<p>We also explored the influence of task difficulty on the way how network represented information. Generally speaking, we found that the more difficult the task, the more integrated the representation of frequency and position. (Results available upon request.)</p>
<p>&nbsp;</p>
<h2 id='introduction'>Introduction</h2>
<p>I have been researching in the neural basis of sequential working memory in the Lab of <a href='http://psy.pku.edu.cn/english/people/faculty/professor/huanluo/index.htm'>Prof. Luo, Huan</a> for nearly a year. I reviewed the literature, selected the research topic, designed the experiment, collected the data and analyzed the result on my own. This experience enabled me to carry out a research project independently and equipped me with many useful tools including experiment presentation, ERP and time-frequency analysis with Psychtoolbox, EEGLAB and Fieldtrip. More details about my project is available <a href='https://github.com/rq-Chen/Undergraduate_Research_at_PKU'>here</a>.</p>
<h2 id='background'>Background</h2>
<p><em><strong>Selective Entrainment of Theta Oscillations in the Dorsal Stream Causally Enhances Auditory Working Memory Performance</strong></em></p>
<ul>
<li><em>Neuron</em>, 2017</li>
<li>DMS (delay matching to sample), melodic, manipulation (reversed order)</li>
<li>mental manipulation ability, MEG/EEG with rhythmic-TMS</li>
<li><strong>Main result: the 5Hz theta oscillation in the Auditory dorsal pathway is causually and positively correlated with the mental manipulation ability (reversing the order).</strong></li>

</ul>
<p><img src=".\expIllu.jpg" referrerpolicy="no-referrer"></p>
<p>&nbsp;</p>
<h2 id='experiment'>Experiment</h2>
<h3 id='intuition'>Intuition</h3>
<p>Although this paper regarded backward replaying as a kind of mental manipulation, this manipulation is apparently special since it has something to do with the temporal organization of the memory contents. Besides, theta oscillation is often associated with temporal organization in the literature. Therefore, probably this result only reflects the neural mechanism for temporal organization instead of mental manipulation. So we want to design another kind of mental manipulation to examine whether this condition triggers different oscillatory pattern with the original ones.</p>
<h3 id='target'>Target</h3>
<ul>
<li>To explore whether this theta oscillation is specifically correlated with the organization of temporal information</li>
<li>To explore the neural mechanism underlying different kinds of mental manipulation for auditory working memory</li>

</ul>
<h3 id='design'>Design</h3>
<p>The pipeline is similar to the original one, but we incorporate two more kinds of mental manipulation in the delay period:</p>
<p><img src = "newExpIllu.jpg" style = "zoom:90%"/></p>
<ul>
<li><p><strong>Transposition</strong></p>
<p>During the delay period, the participants should raise S1 (the sequence they hear) by an octave in their mind. During the reaction period, the participants should compare S2 with the sequence in their mind (the manipulated one).</p>
<p><img src = "transposition.jpg" style = "zoom:40%"/></p>
</li>
<li><p><strong>Contour</strong></p>
<p>During the delay period, participants should categorize S1 acoording to the contour of the melody: “rise-rise” / “rise-fall” / “fall-rise” / “fall-fall”. During the reaction period, they should categorize S2 likewise and compare their categories.</p>
<p><img src = "contour.jpg" style = "zoom:40%"/></p>
</li>

</ul>
<p>&nbsp;</p>
<h2 id='preliminary-result-ongoing'>Preliminary Result (Ongoing)</h2>
<h3 id='behavioral'>Behavioral</h3>
<p><strong>Reaction time:</strong></p>
<p><img src = 'RT.png' style = "zoom:40%" /></p>
<p><strong>Accuracy:</strong></p>
<p><img src = "Accuracy.png" style = "zoom:40%" /></p>
<h3 id='event-related-potential'>Event-related potential</h3>
<p>Cluster-level permutation-based ANOVA over four conditions for delay period ERP amplitude:</p>
<p><img src = 'Bin_F_TP.png' style = "zoom:40%" /></p>
<p>This result was consistent with the reference paper that the left centroparietal channels&#39; ERPs were significantly modulated by task conditions. For comparison see <a href='https://els-jbs-prod-cdn.literatumonline.com/cms/attachment/880b178a-385c-4d53-9ff8-598c72ebbc45/gr2.jpg'>Figure 2</a> in the aforementioned paper.</p>
<h3 id='time-frequency-analysis'>Time-frequency analysis</h3>
<p><strong>Average oscillatory power in the delay period:</strong></p>
<p><img src = "TP_Power.png" style = "zoom:40%" /></p>
<p>Here the power was z-scored against the baseline between -1~0s before the onset of S1. We can see that the four condition generally follows the same activation pattern: mild decrease in global beta power, strong increase in occipital alpha power and no significant change in global theta power. (This will be tested by averaging all data and perform a cluster level t-test later.) Besides, as we predicted, only the REVERSED condition which requires the temporal reorganization of memory contents triggered an increase of theta power. Therefore, we computed the difference between every manipulated condition and the SIMPLE condition and we are currently working on it.</p>
<p><strong>Difference in oscillatory power:</strong></p>
<p><img src = "TP_Power_Diff.png" style = "zoom:40%" /></p>
<p><strong>Time courses of the oscillatory power:</strong></p>
<p><img src = "TF.png" style = "zoom:40%" /></p>
<p>The last two figures are posted just for illustration of our analysis method and we will update the result soon.</p>
<h3 id='decoding-analysis'>Decoding Analysis</h3>
<p>We try to investigate the time course of mental manipulation as well as their neural mechanism through decoding. Since there are 108 kinds of stimuli in our experiment, it&#39;s hard to decode them directly. Therefore, we categorized them using their contour and built an LSTM network to classify the binned EEG signal. We trained the network with a short clip of data and tested it over the whole retention period:</p>
<figure><table>
<thead>
<tr><th>Trained at the beginning</th><th>Trained in the middle</th><th>Trained at the end</th></tr></thead>
<tbody><tr><td><img src="accContourBegin.png" referrerpolicy="no-referrer"></td><td><img src="accContourMiddle.png" referrerpolicy="no-referrer"></td><td><img src="accContourEnd.png" referrerpolicy="no-referrer"></td></tr></tbody>
</table></figure>
<p>We can see that the representation is relatively stable, so the decoder can generalize across a wide range of time. Below is the confusion matrix for the first decoder:</p>
<p><img src = 'tgRDM.gif' style = 'zoom:40%' /></p>
<h2 id='method'>Method</h2>
<ul>
<li>EEG data was collected from 16 participants.</li>
<li>The data was preprocessed in EEGLAB. (<a href='https://github.com/rq-Chen/Undergraduate_Research_at_PKU/tree/master/Auditory%20Working%20Memory/Processing'>Details</a>)</li>
<li>Preliminary behavioral, ERP, time-frequency and decoding analysis was done by Fieldtrip (<a href='https://github.com/rq-Chen/Undergraduate_Research_at_PKU/tree/master/Auditory%20Working%20Memory/Analysis'>Detais</a>):</li>

</ul>
<p>&nbsp;</p>
<div>
    <br>
    <center>
        <span style="float:left">
            <a href="/blogs.html">Blogs</a>
        </span>
        <a href="#top">Back to top</a>
    	<span style="float: right">
    		<a href="/index.html">Homepage</a>
    	</span>
    </center>
</div></body>
</html>